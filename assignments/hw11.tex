\documentclass[letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumerate}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhead[L]{STAT 542 HW11}
\fancyhead[R]{Xin Yin}

\newcommand{\intii}{\int_{-\infty}^\infty}
\newcommand{\Xn}{X_{(n)}}
\newcommand{\intzi}{\int_0^\infty}
\renewcommand{\arraystretch}{1.5}

\begin{document}
\section*{5.13}
Since $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$, we can write $Eg(S^2)$ as $Eg(\frac{\sigma^2}{n-1} \frac{(n-1)S^2}{\sigma^2})$. If we start with $g(s^2) = c\sqrt{s^2}$, we will have,
\begin{align*}
Eg(S^2) & = E(c\sqrt{\frac{\sigma^2}{n-1} \frac{(n-1)S^2}{\sigma^2}}) \\
& = c \sqrt{\frac{\sigma^2}{n-1}} E\sqrt{\frac{(n-1)S^2}{\sigma^2}} \\
\end{align*}
Let $X = \frac{(n-1)s^2}{\sigma^2}$, then $X \sim \chi^2_{n-1}$. Then $E\sqrt{\frac{(n-1)S^2}{\sigma^2}}$ can be written as $E\sqrt{X}$, which is,
\begin{align*}
E\sqrt{X} & = \intzi \sqrt{x} \frac{1}{\Gamma(\frac{n-1}{2})2^{\frac{n-1}{2}}} x^{\frac{n-1}{2} - 1} e^{-\frac{x}{2}} dx \\
& = \frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2}) \sqrt{2}} \intzi \frac{1}{\Gamma(\frac{n}{2}) 2^{\frac{n-1}{2}}} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}} dx 
& = \frac{\Gamma(\frac{n}{2})\sqrt{2}}{\Gamma(\frac{n-1}{2})}
\end{align*}

So,
\begin{align*}
Eg(S^2) & = \frac{\Gamma(\frac{n}{2})\sqrt{2}}{\Gamma(\frac{n-1}{2})} c \sqrt{\frac{1}{n-1}} \sigma = \sigma  \\
\iff & c = \sqrt{\frac{n-1}{2}} \frac{\Gamma(\frac{n-1}{2})}{\Gamma(\frac{n}{2})}
\end{align*}

\section*{5.34}
\begin{align*}
E\frac{\sqrt{n} (\bar X_n - \mu)}{\sigma} = \frac{\sqrt{n}}{\sigma} E(\bar X_n - \mu) = \frac{\sqrt{n}}{\sigma} \mu - \mu = 0 \\
Var \frac{\sqrt{n} (\bar X_n - \mu)}{\sigma} = \frac{n}{\sigma^2} Var (\bar X_n - \mu) = \frac{n}{\sigma^2} \frac{\sigma^2}{n} = 1
\end{align*}
\section*{5.35}
\begin{enumerate}[(a)]
\item Given that $X_i \sim \text{Exp}(1)$, $EX_i = 1, VarX_i = 1$. So, using CLT, we know that,
\[
\frac{\bar X_n -1}{1/\sqrt{n}} \to N(0, 1)
\]
\emph{i.e.}, $\frac{\bar X_n -1}{1/\sqrt{n}} \stackrel{d}{\to} Z$, where $Z$ is a standard normal random variable.

Therefore,
\[
P(\frac{\bar X_n -1}{1/\sqrt{n}} \le x) \to P(Z \le x)
\]
\item 
As $\sqrt{n}(\bar X_n - 1)$ converges to $Z$, it really is the cdf that converges. So,
\begin{align*}
lhs: & \\
\frac{d}{dx} P(\frac{\bar X_n - 1}{1/\sqrt{n}} \le x) & = \frac{d}{dx}P(\bar X_n \frac{x}{\sqrt{n}}+1) = \frac{d}{dx} P(S_n \le x\sqrt{n} + n)\\
& \text{Notice that}~S_n = \sum_{i=1}^n X_i \sim \text{Gamma}(n, 1)\\
& = \frac{d}{dx} F_{S_n}(x\sqrt{n} + n) = \sqrt{n} f_{S_n}(x\sqrt{n}+n) \\
& = \frac{\sqrt{n}}{\Gamma(n)1^n} (x\sqrt{n}+n)^{n-1} e^{-(x\sqrt{n}+n)/1} \\
& = \frac{\sqrt{n}}{\Gamma(n)} (x\sqrt{n}+n)^{n-1} e^{-(x\sqrt{n}+n)}\\
\\
rhs: & \\
\frac{d}{dx} P(Z \le x) & = \frac{d}{dx} F_Z(x) = f_Z(x) \\
& = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
\end{align*}
Hence, 
\[
\frac{\sqrt{n}}{\Gamma(n)} (x\sqrt{n}+n)^{n-1} e^{-(x\sqrt{n}+n)} \approx \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
\]
Let $x = 0$ and notice that $\Gamma(n) = (n-1)!$ for integer $n$, we have,
\[
\frac{\sqrt{n}}{(n-1)!} n^{n-1} e^{-n} \approx \frac{1}{\sqrt{2\pi}}
\]
Rearrange term and multiply both sides with $n$, we will get,
\[
n! \approx \sqrt{2\pi} n^{n+1/2} e^{-n},
\]
which is the Stirling's formula.
\end{enumerate}
\section*{5.36}
\begin{enumerate}[(a)]
\item Since $Y|N \sim \chi^2_{2n}, \quad N \sim \text{Poisson}(\theta)$, we have $E(Y|N) = 2N, Var(Y|N) = 4N, \quad EN = Var N = \theta$
\[
EY = E(E(Y|N)) = E(2N) = 2\theta
\]
\[
Var Y = E(Var(Y|N)) + Var(E(Y|N)) = E(4N) + Var(2N) = 8\theta
\]
\item The mgf of random variable $Y$ is, $M_Y(t) = Ee^{tY}$. Using the formula that $Eg(Y) = E(E(g(Y)|N))$, given that $g(Y) = e^{tY}$, we have,
\[
M_Y(t) = Ee^{tY} = E(E(e^{tY}|N)),
\]
where $E(e^{tY}|N)$ is the mgf of $Y|N$, which is the mgf for $\chi^2_{2n}$, \emph{i.e.}, $M_{Y|N}(t) = E(e^{tY}|N) = \left(\frac{1}{1-2t} \right)^N$. Therefore,
\begin{align*}
M_Y(t) & = E(E(e^{tY}|N)) = E\left(\frac{1}{1-2t}\right)^N \\
& = \sum_{n=0}^n \left(\frac{1}{1-2t}\right)^n \frac{e^{-\theta} \theta^n}{n!} = e^{-\theta}\sum_{n=0}^n \frac{\left(\frac{\theta}{1-2t}\right)^n}{n!} \\
& = e^{-\theta} e^{\frac{\theta}{1-2t}} = e^{\frac{2t\theta}{1-2t}}
\end{align*}

Now, the mgf of $(Y-EY)/\sqrt{VarY}$ is, if we let $U = (Y-EY)/\sqrt{VarY} = \frac{Y}{\sqrt{8\theta}} - \frac{\sqrt{\theta}}{\sqrt{2}}$,
\begin{align*}
M_U(t) & = e^{-\frac{\sqrt{\theta}}{\sqrt{2}}t} M_Y(\frac{t}{\sqrt{8\theta}}) \\
& = e^{-\frac{2\theta}{\sqrt{8\theta}}t} e^{\frac{\theta t}{\sqrt{2\theta} - t}} = e^{\frac{t^2}{2-t\sqrt{2/\theta}}} \\
\end{align*}
Since $\lim_{\theta \to \infty} e^{\frac{t^2}{2-t\sqrt{2/\theta}}} = e^{\frac{t^2}{2}}$ is the mgf for standard normal distribution, we have that,
\[
(Y-EY)/sqrt{VarY} \stackrel{d}{\to} N(0, 1)
\]

\end{enumerate}
\section*{5.38}
\begin{enumerate}[(a)]
\item Let random variable $W = S_n - a$, we can see that, $e^{-at}[M_X(t)]^n = e^{-at}M_{S_n}(t) = M_{S_n - a}(t) = M_W(t)$. Using the definition of mgf, we have,
\[
M_W(t) = \intii e^{tw} f_W(w) dw \ge \int_{w: w >0} e^{tw} f_W(w) dw
\]
Since mgf exists for $-h < t < h$, for $0 < t < h, \quad w > 0$ guarantee that $e^{tw} > 1$, therefore,
\begin{align*}
M_W(t) & = \intii e^{tw} f_W(w) dw \ge \int_{w: w >0} e^{tw} f_W(w) dw \\
 & \ge \int_{w: w>0} f_W(w) dw, \quad 0 < t < h \\
 & = P(W > 0) = P(S_n > a), \quad 0 < t < h
\end{align*}
Hence, $P(S_n > a) \le e^{-at}[M_X(t)]^n$.

Similarly,
\begin{align*}
M_W(t) & = \intii e^{tw} f_W(w) dw \ge \int_{w: w \le 0} e^{tw} f_W(w) dw \\
 & \ge \int_{w: w \le 0} f_W(w) dw, \quad -h < t < 0 \\
 & = P(W \le 0) = P(S_n \le a), \quad -h < t < 0
\end{align*}
which implies, $P(S_n \le a) \le e^{-at}[M_X(t)]^n$.
\item 
Given that $M_X(0) = 1$, $M_X'(0) = E(X) < 0$, we have,
\[
\lim_{t\to 0}\frac{M_X(t) - M_X(0)}{t-0} = \lim_{t \to 0}M_X'(t) = E(X) < 0
\]
So, $M_X(t) = M_X(0) + tE(X) = 1 + tE(X) < 1$, given that $E(X) < 0$. 
Then, let $c = 1 + tE(X), 0 < c < 1$, 
\[
P(S_n > a) \le e^{-at}[M_X(t)]^n = e^{-at}c^n
\]
For $a > 0, e^{-at} < 1$, therefore for $n$ large enough, there exists $0 < c<1$, such that,
\[
P(S_n > a) \le c^n
\]

Similarly, for $P(S_n \le a)$, we will have $P(S_n \le a) \le c^n$, for some $0 < c <1$, when $E(X) > 0$, as $t < 0$ in this case.

\item $\sum_{i=1}^n Y_i = \sum_{i=1}^n (X_i - \mu - \epsilon) = S_n - n\mu - n\epsilon$. So, using above argument, with $a = 0$, we have,
\[
P(\sum_{i=1}^n Y_i > a) = P(S_n - n\mu - n\epsilon > 0) = P(S_n - n\mu > n\epsilon) = P(\bar X_n - \mu > \epsilon) \le c^n,
\]
where c is a constant satisfying that $0<c<1$.
\item Now set $Y_i = -X_i + \mu - \epsilon$, so $\sum_{i=1}^n Y_i = -S_n + n\mu - n\epsilon$. Using the result from (b), with $a = 0$, we have,
\[
P(\sum_{i=1}^n Y_i > a) = P(-\bar X_n + \mu > \epsilon) \le c^n.
\]

Now assume that $P(-\bar X_n + \mu > \epsilon) \le c_1^n$, $P(\bar X_n - \mu > \epsilon) \le c_2^n$, we can see that,
\[
P(|\bar X_n - \mu| > \epsilon) = P(-\bar X_n + \mu > \epsilon) + P(\bar X_n - \mu > \epsilon) \le c_1^n + c_2^n \le 2 [\max\{c_1, c_2\}]^n
\]
So, just set $\forall c = [\max\{c_1, c_2\}, 1)$, which still satisfies $0 < c < 1$, then we have,
\[
P(|\bar X_n - \mu| > \epsilon) < 2c^n, \text{for some}~0 < c < 1.
\]

\end{enumerate}
\section*{5.42}
\begin{enumerate}[(a)]
\item First we look at $P(n^{\upsilon}(1-X_i) \le x)$, which is,
\begin{align*}
P(n^{\upsilon}(1-X_i) \le x) & = P(X_i \ge 1 - \frac{x}{n^\upsilon}) = 1-\int_0^1 \frac{\Gamma(1+\beta)}{\Gamma(1)\Gamma(1+\beta)} \left(1-\frac{x}{n^\upsilon}\right)^0 \left(\frac{x}{n^\upsilon}\right)^{\beta-1} \\
& = 1 - x^\beta n^{-\beta\upsilon} 
\end{align*}
So, 
\[
P(n^{\upsilon}(1-\Xn) \le x) = P(\Xn \ge 1 - \frac{x}{n^\upsilon}) = (1- x^\beta n^{-\beta\upsilon})^n
\]
One way to let above expression to converge, is to find $\upsilon$ such that above expression can be written as $(1-\frac{x^\beta}{n})^n \to e^{-x^\beta}$, as $n\to\infty$. And we can easily identify that $\upsilon = \frac{1}{\beta}$.
\item Since $X_i \sim \text{Exp}(1)$,
\[
P(X_i - a_n \le x) = P(X_i \le x + a_n) = 1 - e^{-(x+a_n)} = 1 - \frac{e^{-x}}{e^{a_n}}
\]
Then,
\[
P(\Xn - a_n \le x) = P(\Xn \le x + a_n) = \left(1-\frac{e^{-x}}{e^{a_n}}\right)^n
\]
As we can see, if we set $a_n = \log(n)$, above expression can be written as $\left(1-\frac{e^{-x}}{n}\right)^n \to e^{-e^{-x}}$, as $n \to \infty$.
\end{enumerate}
\section*{5.43}
\begin{enumerate}[(a)]
\item We can check that,
\[
P(|Y_n - \mu| < \epsilon) = P(|\sqrt{n}(Y_n - \mu)| < \sqrt{n} \epsilon).
\]
Since $\sqrt{n}(Y_n - \mu) \to Z$ where $Z \sim N(0, \sigma^2)$, 
\[
\lim_{n\to\infty} P(|\sqrt{n}(Y_n - \mu)| < \sqrt{n} \epsilon) = P(|Z| < \infty) = 1
\]
Therefore, $\lim_{n\to\infty} P(|Y_n - \theta| < \epsilon) = 1$, \emph{i.e.}, $Y_n \stackrel{p}{\to} \mu$.
\item In the context of proof of Theorem 5.5.24, since we already know that $\sqrt{n}(Y_n - \theta) \stackrel{d}{\to} N(0, \sigma^2)$, using Slutsky's theorem, $g'(\theta)\sqrt{n}(Y_n - \theta) \stackrel{d}{\to} g'(\theta) Z$ where $Z \sim N(0, \sigma^2)$. 

Therefore, $\sqrt{n}(g(Y_n) - g(\theta)) = g'(\theta)\sqrt{n}(Y_n - \theta) \stackrel{d}{\to} N(0, \sigma^2[g'(\theta)]^2)$.
\end{enumerate}

\section*{5.44}
\begin{enumerate}[(a)]
\item Since $X_i \stackrel{iid}{\sim} \text{Ber}(p), \mu_{X_i} = p, \sigma_{X_i}^2 = p(1-p)$. Then, using CLT, we know that, for sample mean $Y_n = \frac{1}{n} \sum_{i=1}^n X_i$, it satisfies,
\[
\sqrt{n}(Y_n - p) = \sqrt{n}(Y_n - \mu_{X_i}) \stackrel{d}{\to} N(0, \sigma^2_{X_i}) = N(0, p(1-p))
\]
\item Define $g(x) = x(1-x)$. Then, $g(Y_n) = Y_n(1-Y_n), \quad g(\mu_{X_i}) = \mu_{X_i} (1-\mu_{X_i}) = p(1-p)$. 
Since $p \ne 1/2$, $g'(\mu_{X_i}) = (1-p) - p = 1-2p$ exists and is not 0. And we can use delta method to show that,
\[
\sqrt{n}(g(Y_n) - g(\mu_{X_i}) = \sqrt{n}(Y_n(1-Y_n) - p(1-p)) \stackrel{d}{\to} N(0, \sigma_{X_i}^2 [g'(\mu_{X_i})]^2) = N(0, p(1-p) (1-2p)^2)
\]

\item If $p = 1/2$, $\sigma_{X_i}^2 = p(1-p) = \frac{1}{4}$. Notice that $g'(\mu_{X_i} 1-2p = 0$, we can then use second-order delta method to show that,
\[
n[g(Y_n) - g(\mu_{X_i})] = n(Y_n(1-Y_n) - \frac{1}{4}) \stackrel{d}{\to} \sigma^2_{X_i} \chi_1^2 = \frac{1}{4} \chi_1^2.
\]
\end{enumerate}
\end{document}
